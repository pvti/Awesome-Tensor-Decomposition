# [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) Tensor Decomposition 
A curated list of tensor decomposition resources for *network compression*.

## :clipboard: Research Papers
| Title                                                        | Venue      | Year |
| ------------------------------------------------------------ | ---------- | ---- |
| [PELA: Learning Parameter-Efficient Models with Low-Rank Approximation](https://arxiv.org/abs/2310.10700) [![GitHub Repo stars](https://img.shields.io/github/stars/guoyang9/PELA)](https://github.com/guoyang9/PELA)| CVPR | 2024 |
| [Adaptive Rank Selections for Low-Rank Approximation of Language Models](https://aclanthology.org/2024.naacl-long.13/)| ACL | 2024 |
| [Learning Low-Rank Tensor Cores with Probabilistic l0-Regularized Rank Selection for Model Compression](https://www.ijcai.org/proceedings/2024/418) [![GitHub Repo stars](https://img.shields.io/github/stars/ctxGou/Tensor-L0-Compression)](https://github.com/ctxGou/Tensor-L0-Compression)| IJCAI | 2024 |
| [Compact Model Training by Low-Rank Projection With Energy Transfer](https://ieeexplore.ieee.org/document/10551437) [![GitHub Repo stars](https://img.shields.io/github/stars/BZQLin/LRPET)](https://github.com/BZQLin/LRPET)| TNNLS | 2024 |
| [An Accuracy-Preserving Neural Network Compression via Tucker Decomposition](https://ieeexplore.ieee.org/document/10614384)| IEEE Transactions on Sustainable Computing | 2024 |
| [Co-Exploring Structured Sparsification and Low-Rank Tensor Decomposition for Compact DNNs](https://ieeexplore.ieee.org/document/10574865)| TNNLS | 2024 |
| [Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization](https://arxiv.org/abs/2406.12930)| ISCA | 2024 |
| [Coarse-To-Fine Tensor Trains for Compact Visual Representations](https://arxiv.org/abs/2406.04332) [![GitHub Repo stars](https://img.shields.io/github/stars/sebulo/PuTT)](https://github.com/sebulo/PuTT)| ICML | 2024 |
| [Position: Tensor Networks are a Valuable Asset for Green AI](https://openreview.net/pdf?id=mcg6jppkwb)| ICML | 2024 |
| [Compression-aware Training of Neural Networks using Frank-Wolfe](https://arxiv.org/abs/2205.11921) [![GitHub Repo stars](https://img.shields.io/github/stars/ZIB-IOL/compression-aware-SFW)](https://github.com/ZIB-IOL/compression-aware-SFW)| Arxiv | 2024 |
| [Unified Low-rank Compression Framework for Click-through Rate Prediction](https://arxiv.org/abs/2405.18146) [![GitHub Repo stars](https://img.shields.io/github/stars/yuhao318/atomic_feature_mimicking)](https://github.com/yuhao318/atomic_feature_mimicking)| KDD2024 ADS | 2024 |
| [A Practical Approach for Employing Tensor Train Decomposition in Edge Devices](https://link.springer.com/article/10.1007/s10766-024-00762-3) | IJPP | 2024 |
| [Structure-Preserving Network Compression Via Low-Rank Induced Training Through Linear Layers Composition](https://arxiv.org/abs/2405.03089) | Arxiv | 2024 |
| [LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models](https://arxiv.org/abs/2402.11417) [![GitHub Repo stars](https://img.shields.io/github/stars/yifanycc/loretta)](https://github.com/yifanycc/loretta)| NAACL | 2024 |
| [CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization](https://arxiv.org/abs/2405.14377) | Arxiv | 2024 |
| [FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer](https://arxiv.org/abs/2311.03912) [![GitHub Repo stars](https://img.shields.io/github/stars/shadowpa0327/FLORA)](https://github.com/shadowpa0327/FLORA)| WACV | 2024 |
| [FLoRA: Low-Rank Core Space for N-dimension](https://arxiv.org/abs/2405.14739) [![GitHub Repo stars](https://img.shields.io/github/stars/SJTU-DeepVisionLab/FLoRA)](https://github.com/SJTU-DeepVisionLab/FLoRA)| Arxiv | 2024 |
| [Reduced storage direct tensor ring decomposition for convolutional neural networks compression](https://arxiv.org/abs/2405.10802) [![GitHub Repo stars](https://img.shields.io/github/stars/mateuszgabor/rsdtr_compression)](https://github.com/mateuszgabor/rsdtr_compression)| Arxiv | 2024 |
| [Federated Learning Using Coupled Tensor Train Decomposition](https://arxiv.org/abs/2403.02898) | Arxiv | 2024 |
| [Neural Network Compression Based on Tensor Ring Decomposition](https://ieeexplore.ieee.org/abstract/document/10510501) | TNNLS | 2024 |
| [Enhanced network compression through tensor decompositions and pruning](https://hal.science/hal-04475167v1) [![GitHub Repo stars](https://img.shields.io/github/stars/pvti/NORTON)](https://github.com/pvti/NORTON)| TNNLS | 2024 |
| [Enhancing GAN Performance Through Neural Architecture Search and Tensor Decomposition](https://ieeexplore.ieee.org/document/10446488) [![GitHub Repo stars](https://img.shields.io/github/stars/PrasannaPulakurthi/MMD-AdversarialNAS)](https://github.com/PrasannaPulakurthi/MMD-AdversarialNAS)| ICASSP | 2024 |
| [Deep Convolutional Neural Network Compression Method: Tensor Ring Decomposition with Variational Bayesian Approach](https://link.springer.com/article/10.1007/s11063-024-11465-8) | Neural Processing Letters | 2024 |
| [Compressing Transformers: Features Are Low-Rank, but Weights Are Not!](https://ojs.aaai.org/index.php/AAAI/article/view/26304) | AAAI | 2023 |
| [Deep Learning Model Compression With Rank Reduction in Tensor Decomposition](https://ieeexplore.ieee.org/abstract/document/10321737) | TNNLS | 2023 |
| [MARS: Masked Automatic Ranks Selection in Tensor Decompositions](https://arxiv.org/abs/2006.10859) [![GitHub Repo stars](https://img.shields.io/github/stars/MaxBourdon/mars)](https://github.com/MaxBourdon/mars)| AISTATS | 2023 |
| [Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition](https://arxiv.org/abs/2306.05021) [![GitHub Repo stars](https://img.shields.io/github/stars/Yu-Zhewen/Mixed-TD)](https://github.com/Yu-Zhewen/Mixed-TD)| FPL | 2023 |
| [SVD-NAS: Coupling Low-Rank Approximation and Neural Architecture Search](https://arxiv.org/abs/2208.10404) [![GitHub Repo stars](https://img.shields.io/github/stars/Yu-Zhewen/SVD-NAS)](https://github.com/Yu-Zhewen/SVD-NAS)| WACV | 2023 |
| [How Informative is the Approximation Error from Tensor Decomposition for Neural Network Compression?](https://arxiv.org/abs/2305.05318) | ICLR | 2023 |
| [Tensor shape search for efficient compression of tensorized data and neural networks](https://www.sciencedirect.com/science/article/pii/S1568494623010050) | Applied Soft Computing | 2023 |
| [FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer](https://arxiv.org/abs/2212.03145) | AAAI | 2023 |
| [Compressing convolutional neural networks with hierarchical Tucker-2 decomposition](https://www.sciencedirect.com/science/article/pii/S156849462200905X) [![GitHub Repo stars](https://img.shields.io/github/stars/mateuszgabor/ht2)](https://github.com/mateuszgabor/ht2)| Applied Soft Computing | 2023 |
| [Tensor shape search for efficient compression of tensorized data and neural networks](https://www.sciencedirect.com/science/article/pii/S1568494623010050) | Applied Soft Computing | 2023 |
| [An effective low-rank compression with a joint rank selection followed by a compression-friendly training](https://www.sciencedirect.com/science/article/pii/S0893608023000242) | Neural Networks | 2023 |
| [Joint matrix decomposition for deep convolutional neural networks compression](https://www.sciencedirect.com/science/article/pii/S0925231222012887) [![GitHub Repo stars](https://img.shields.io/github/stars/ShaowuChen/JointSVD)](https://github.com/ShaowuChen/JointSVD)| Neurocomputing | 2023 |
| [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization](https://arxiv.org/abs/2309.03824) | Arxiv | 2023 |
| [Knowledge Transfer via Decomposing Essential Information in Convolutional Neural Networks](https://ieeexplore.ieee.org/document/9222552) [![GitHub Repo stars](https://img.shields.io/github/stars/sseung0703/KD_methods_with_TF)](https://github.com/sseung0703/KD_methods_with_TF)| TNNLS | 2022 |
| [Compression of Deep Neural Networks based on quantized tensor decomposition to implement on reconfigurable hardware platforms](https://www.sciencedirect.com/science/article/pii/S089360802200065X) | Neural Networks | 2022 |
| [Teacherâ€“student knowledge distillation based on decomposed deep feature representation for intelligent mobile applications](https://doi.org/10.1016/j.eswa.2022.117474) | Expert Systems with Applications | 2022 |
| [HODEC: Towards Efficient High-Order DEcomposed Convolutional Neural Networks](https://ieeexplore.ieee.org/document/9879408) | CVPR | 2022 |
| [Towards Practical Control of Singular Values of Convolutional Layers](https://arxiv.org/abs/2211.13771) [![GitHub Repo stars](https://img.shields.io/github/stars/WhiteTeaDragon/practical_svd_conv)](https://github.com/WhiteTeaDragon/practical_svd_conv)| NeurIPS | 2022 |
| [Strategies for Applying Low Rank Decomposition to Transformer-Based Models](https://neurips2022-enlsp.github.io/papers/paper_33.pdf) | NeurIPS | 2022 |
| [Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations](https://arxiv.org/abs/2205.13571) [![GitHub Repo stars](https://img.shields.io/github/stars/COMPiLELab/DLRT-Net)](https://github.com/COMPiLELab/DLRT-Net)| NeurIPS | 2022 |
| [BATUDE: Budget-Aware Neural Network Compression Based on Tucker Decomposition](https://ojs.aaai.org/index.php/AAAI/article/view/20869) | AAAI | 2022 |
| [Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition](https://ojs.aaai.org/index.php/AAAI/article/view/19958) | AAAI | 2022 |
| [Towards Compact Neural Networks via End-to-End Training: A Bayesian Tensor Approach with Automatic Rank Determination](https://arxiv.org/abs/2010.08689) [![GitHub Repo stars](https://img.shields.io/github/stars/colehawkins/bayesian-tensor-rank-determination)](https://github.com/colehawkins/bayesian-tensor-rank-determination)| SIMODS | 2022 |
| [A novel compact design of convolutional layers with spatial transformation towards lower-rank representation for image classification](https://doi.org/10.1016/j.knosys.2022.109723) [![GitHub Repo stars](https://img.shields.io/github/stars/liubc17/low_rank_compact_transformed)](https://github.com/liubc17/low_rank_compact_transformed)| Knowledge-Based Systems | 2022 |
| [Deep neural network compression by Tucker decomposition with nonlinear response](https://www.sciencedirect.com/science/article/pii/S0950705122000326) | Knowledge-Based Systems | 2022 |
| [Nested compression of convolutional neural networks with Tucker-2 decomposition](https://ieeexplore.ieee.org/abstract/document/9892959) | IJCNN | 2022 |
| [PSM-nets: Compressing Neural Networks with Product of Sparse Matrices](https://ieeexplore.ieee.org/abstract/document/9533408) | IJCNN | 2022 |
| [A Design Space Exploration Methodology for Enabling Tensor Train Decomposition in Edge Devices](https://link.springer.com/chapter/10.1007/978-3-031-15074-6_11) | SAMOS | 2022 |
| [Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition](https://arxiv.org/abs/2107.11442) [![GitHub Repo stars](https://img.shields.io/github/stars/lucaslie/torchprune)](https://github.com/lucaslie/torchprune)| NeurIPS | 2021 |
| [Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks](https://arxiv.org/abs/2006.05066) [![GitHub Repo stars](https://img.shields.io/github/stars/ssregibility/Net_RL2)](https://github.com/ssregibility/Net_RL2)| NeurIPS | 2021 |
| [Parameter Efficient Dynamic Convolution via Tensor Decomposition](https://www.bmvc2021-virtualconference.com/conference/papers/paper_1631.html) [![GitHub Repo stars](https://img.shields.io/github/stars/zejiangh/PEDConv)](https://github.com/zejiangh/PEDConv)| BMVC | 2021 |
| [Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework](https://ieeexplore.ieee.org/document/9577813) | CVPR | 2021 |
| [Deep Convolutional Neural Network Compression via Coupled Tensor Decomposition](https://ieeexplore.ieee.org/document/9261106) | JSTSP | 2021 |
| [Tensor Reordering for CNN Compression](https://arxiv.org/abs/2010.12110) | ICASSP | 2021 |
| [Block-term tensor neural networks](https://www.sciencedirect.com/science/article/pii/S0893608020302045)| Neural Networks | 2020 |
| [Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer](https://ieeexplore.ieee.org/document/9157223) [![GitHub Repo stars](https://img.shields.io/github/stars/UCMerced-ML/LC-model-compression)](https://github.com/UCMerced-ML/LC-model-compression)| CVPR | 2020 |
| [Learning Low-rank Deep Neural Networks via Singular Vector Orthogonality Regularization and Singular Value Sparsification](https://arxiv.org/abs/2004.09031) [![GitHub Repo stars](https://img.shields.io/github/stars/yanghr/SVD_Prune_EDLCV)](https://github.com/yanghr/SVD_Prune_EDLCV)| CVPRW | 2020 |
| [PENNI: Pruned Kernel Sharing for Efficient CNN Inference](https://arxiv.org/abs/2005.07133) [![GitHub Repo stars](https://img.shields.io/github/stars/timlee0212/PENNI)](https://github.com/timlee0212/PENNI)| ICML | 2020 |
| [A Novel Rank Selection Scheme in Tensor Ring Decomposition Based on Reinforcement Learning for Deep Neural Networks](https://ieeexplore.ieee.org/document/9053292)| ICASSP | 2020 |
| [Holistic CNN Compression via Low-Rank Decomposition with Knowledge Transfer](https://ieeexplore.ieee.org/document/8478366) [![GitHub Repo stars](https://img.shields.io/github/stars/ShaohuiLin/LRDKT)](https://github.com/ShaohuiLin/LRDKT)| TPAMI | 2019 |
| [LTNN: A Layerwise Tensorized Compression of Multilayer Neural Network](https://ieeexplore.ieee.org/document/8480873)| TNNLS | 2019 |
| [Efficient Neural Network Compression](https://arxiv.org/abs/1811.12781) [![GitHub Repo stars](https://img.shields.io/github/stars/Hyeji-Kim/ENC)](https://github.com/Hyeji-Kim/ENC)| CVPR | 2019 |
| [ADA-Tucker: Compressing deep neural networks via adaptive dimension adjustment tucker decomposition](https://www.sciencedirect.com/science/article/pii/S0893608018303010) | Neural Networks | 2019 |
| [Learning Filter Basis for Convolutional Neural Network Compression](https://arxiv.org/abs/1908.08932) [![GitHub Repo stars](https://img.shields.io/github/stars/ofsoundof/learning_filter_basis)](https://github.com/ofsoundof/learning_filter_basis)| ICCV | 2019 |
| [Automated Multi-Stage Compression of Neural Networks](https://ieeexplore.ieee.org/document/9022596) [![GitHub Repo stars](https://img.shields.io/github/stars/musco-ai/musco-pytorch)](https://github.com/musco-ai/musco-pytorch)| ICCVW | 2019 |
| [Compressing Deep Models using Multi Tensor Train Decomposition](https://ieeexplore.ieee.org/document/9074612) | ICCAIS | 2019 |
| [Compressing Fully Connected Layers using Kronecker Tensor Decomposition](https://ieeexplore.ieee.org/document/8962432) | ICCSNT | 2019 |
| [Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling](https://openreview.net/forum?id=r1xFE3Rqt7) [![GitHub Repo stars](https://img.shields.io/github/stars/zuenko/ALRF)](https://github.com/zuenko/ALRF)| OpenReview | 2019 |
| [Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition](https://arxiv.org/abs/1712.05134) | CVPR | 2018 |
| [Wide Compression: Tensor Ring Nets](https://arxiv.org/abs/1802.09052) | CVPR | 2018 |
| [Self-supervised Knowledge Distillation Using Singular Value Decomposition](https://arxiv.org/abs/1807.06819) [![GitHub Repo stars](https://img.shields.io/github/stars/sseung0703/SSKD_SVD)](https://github.com/sseung0703/SSKD_SVD)| ECCV | 2018 |
| [Extreme Network Compression via Filter Group Approximation](https://arxiv.org/abs/1807.11254) | ECCV | 2018 |
| [Network Decoupling: From Regular to Depthwise Separable Convolutions](https://arxiv.org/abs/1808.05517) [![GitHub Repo stars](https://img.shields.io/github/stars/JianboGuo/network-decoupling)](https://github.com/JianboGuo/network-decoupling)| BMVC | 2018 |
| [On Compressing Deep Models by Low Rank and Sparse Decomposition](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_On_Compressing_Deep_CVPR_2017_paper.pdf) | CVPR | 2017 |
| [Coordinating Filters for Faster Deep Neural Networks](https://arxiv.org/abs/1703.09746) [![GitHub Repo stars](https://img.shields.io/github/stars/wenwei202/caffe)](https://github.com/wenwei202/caffe)| ICCV | 2017 |
| [Factorized Convolutional Neural Networks](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w10/Wang_Factorized_Convolutional_Neural_ICCV_2017_paper.pdf) | ICCVW | 2017 |
| [Accelerating Very Deep Convolutional Networks for Classification and Detection](https://arxiv.org/abs/1505.06798) | TPAMI | 2016 |
| [Convolutional Neural Networks With Low-rank Regularization](https://arxiv.org/abs/1511.06067) [![GitHub Repo stars](https://img.shields.io/github/stars/chengtaipu/lowrankcnn)](https://github.com/chengtaipu/lowrankcnn)| ICLR | 2016 |
| [Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications](https://arxiv.org/abs/1511.06530) | ICLR | 2016 |
| [Towards Convolutional Neural Networks Compression via Global Error Reconstruction](https://www.ijcai.org/Proceedings/16/Papers/251.pdf) | IJCAI | 2016 |
| [Accelerating Convolutional Neural Networks for Mobile Applications](https://doi.org/10.1145/2964284.2967280) | MM | 2016 |
| [Ultimate tensorization: compressing convolutional and FC layers alike](https://arxiv.org/abs/1611.03214) [![GitHub Repo stars](https://img.shields.io/github/stars/timgaripov/TensorNet-TF)](https://github.com/timgaripov/TensorNet-TF) | NIPSW | 2016 |
| [Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition](https://arxiv.org/abs/1412.6553) | ICLR | 2015 |
| [Speeding up Convolutional Neural Networks with Low Rank Expansions](https://arxiv.org/abs/1405.3866) | Arxiv | 2014 |

## :books: Surveys
| Title                                                        | Venue      | Year |
| ------------------------------------------------------------ | ---------- | ---- |
| [Low Rank Optimization for Efficient Deep Learning: Making a Balance Between Compact Architecture And Fast Training](https://ieeexplore.ieee.org/document/10355073) | Journal of Systems Engineering and Electronics | 2024 |
| [Tensor Decomposition for Model Reduction in Neural Networks: A Review](https://arxiv.org/abs/2304.13539) | IEEE Circuits and Systems Magazine | 2023 |
| [Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training](https://arxiv.org/abs/2303.13635)| Arxiv | 2023 |
| [Tensor Networks Meet Neural Networks: A Survey and Future Perspectives](https://arxiv.org/abs/2302.09019) [![GitHub Repo stars](https://img.shields.io/github/stars/tnbar/awesome-tensorial-neural-networks)](https://github.com/tnbar/awesome-tensorial-neural-networks)| Arxiv | 2023 |
| [High-performance tensor decompositions for compressing and accelerating deep neural networks](https://www.sciencedirect.com/science/article/abs/pii/B9780128244470000157) [![GitHub Repo stars](https://img.shields.io/github/stars/YangletLiu/Tensor_Layer_for_Deep_Neural_Network_Compression)](https://github.com/YangletLiu/Tensor_Layer_for_Deep_Neural_Network_Compression)| Tensors for Data Processing | 2022 |
| [Tensor Decomposition for Signal Processing and Machine Learning](https://ieeexplore.ieee.org/abstract/document/7891546) | IEEE Transactions on Signal Processing | 2017 |
| [The Higher-Order Singular Value Decomposition: Theory and an Application](https://ieeexplore.ieee.org/document/5447070) | IEEE Signal Processing Magazine | 2010 |
| [Tensor Decompositions and Applications](https://epubs.siam.org/doi/10.1137/07070111X) | SIAM Review | 2009 |

## :blue_book: Miscellaneous
| Title                                                        | Venue      | Year |
| ------------------------------------------------------------ | ---------- | ---- |
| [Nuclear Norm of Higher-Order Tensors](https://arxiv.org/abs/1410.6072) | Mathematics of Computation | 2018 |

## :computer: Repositories
- [TensorLy](https://github.com/tensorly/tensorly) [![GitHub Repo stars](https://img.shields.io/github/stars/tensorly/tensorly)](https://github.com/tensorly/tensorly)
- [TensorLy-Torch](https://tensorly.org/torch/dev/) [![GitHub Repo stars](https://img.shields.io/github/stars/tensorly/torch)](https://github.com/tensorly/torch)
- [PyTorch Tensor Decompositions](https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning)[![GitHub Repo stars](https://img.shields.io/github/stars/jacobgil/pytorch-tensor-decompositions)](https://github.com/jacobgil/pytorch-tensor-decompositions)
- [CNN_compression_with_Tensor_Decomposition](https://github.com/K0EKJE/CNN_compression_with_Tensor_Decomposition)[![GitHub Repo stars](https://img.shields.io/github/stars/K0EKJE/CNN_compression_with_Tensor_Decomposition)](https://github.com/K0EKJE/CNN_compression_with_Tensor_Decomposition)
- [Tensor methods in Python with TensorLy](https://github.com/JeanKossaifi/tensorly-notebooks)[![GitHub Repo stars](https://img.shields.io/github/stars/JeanKossaifi/tensorly-notebooks)](https://github.com/JeanKossaifi/tensorly-notebooks)
- [TensorKrowch: Smooth integration of tensor networks in machine learning](https://arxiv.org/abs/2306.08595)[![GitHub Repo stars](https://img.shields.io/github/stars/joserapa98/tensorkrowch)](https://github.com/joserapa98/tensorkrowch)
- [Tensor Learning Team](https://qibinzhao.github.io/)
